---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.1.3
  kernelspec:
    display_name: Python [conda env:pytorch] *
    language: python
    name: conda-env-pytorch-py
---

```{python}
import sys
import os

if os.getcwd().split('/')[-1]!='dsbowl':
    os.chdir('dsbowl')

import random
import warnings

import matplotlib.pyplot as plt
import numpy as np
import cv2

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch.nn.functional import binary_cross_entropy

from modules.dataset import load_train_data, load_test_data
from modules.preds import create_submission
from modules.metrics import mean_iou
from modules.transforms import get_train_tfms, get_test_tfms
from modules.transforms_functional import tensor_to_img
from modules.nets import Net, OneCycleScheduler
from modules.model_factory import get_model
from modules.files import get_sizes, getNextFilePath
import config as cfg
```

```{python}
tl, vl = load_train_data(cfg.TRAIN_PATH, size=cfg.TRAIN_SIZE, bs=cfg.BATCH_SIZE,
                         transforms=get_train_tfms(cfg.TRAIN_SIZE), use_augs=True, shuffle=False)
```

```{python}
ts = tl.dataset
```

```{python}
ts.show(5, transformed=True)
```

```{python}
ts.show(5, transformed=False)
```

```{python}
dls = {'train': tl, 'val': vl}
```

```{python}
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

mod = get_model(cfg.MODEL, cfg.CLASSES, act=cfg.ACT)
opt = optim.Adam(mod.parameters(), lr=cfg.LRS[0], weight_decay=cfg.WD)
net = Net(mod, opt, nn.BCELoss(), [mean_iou], cfg.MODELS_PATH)
```

```{python}
scheduler = OneCycleScheduler(cfg.LRS, len(tl))
save_name = f'{cfg.MODEL}_{cfg.EPOCHS}_{cfg.LRS[0]}_{cfg.WD}'
save_name += f'_{getNextFilePath(cfg.MODELS_PATH, save_name)}'
print(save_name)
```

```{python}
writer = SummaryWriter(log_dir='/work/stages/schwob/runs/'+save_name)
```

```{python}
writer.add_graph(mod, input_to_model=next(iter(tl))[0], operator_export_type='RAW')
```

```{python}
net.fit(dls, cfg.EPOCHS, save_name, device, writer=writer, scheduler=scheduler)
```

```{python}
net.load("resnet34_10_0.0002_0.pth")
```

```{python}
X, y_true = ts[3]
```

```{python}
y_pred = net.model(X).detach()
```

```{python}
img = tensor_to_img(X)
mask_true = tensor_to_img(y_true).squeeze()
mask_pred = tensor_to_img(y_pred).squeeze()
plt.figure(0, (15, 15))
plt.subplot(131)
plt.axis('off')
plt.imshow(img)
plt.subplot(132)
plt.axis('off')
plt.imshow(mask_true)
plt.subplot(133)
plt.axis('off')
plt.imshow(mask_pred>0.5)
```

```{python}
print(mean_iou(y_pred, y_true))
print(binary_cross_entropy(y_pred, y_true))
```

```{python}
testloader = load_test_data(cfg.TEST_PATH, size=cfg.MAX_SIZE, bs=cfg.BATCH_SIZE, transforms=get_test_tfms(cfg.MAX_SIZE))
testset = testloader.dataset
```

```{python}
ids = testset.ids
sizes = get_sizes(cfg.TEST_CSV, ids)
```

```{python}
preds = net.predict(testloader, device, sizes, size=cfg.TEST_SIZE, overlap=cfg.TEST_OVERLAP, out_channels=cfg.CLASSES)
```

```{python}
create_submission(preds, sizes, ids, folder=cfg.SUB_PATH)
```

```{python}
a = 'ABCDE'
```

```{python}
import sys
s = 'a'
t = 'a'
sys.getsizeof(s[-1])
```

```{python}
k = 0
s='a'
t='a'
while s is t:
    k += 1
    s = 'a'*k
    t = 'a'*k
k
```

```{python}
k=2
s='a'*k
t='a'*k
s is t
```

```{python}
class c:
    def __init__(self):
        self.a = 1
d = c()
sys.getsizeof(d)
```

```{python}
from string import ascii_lowercase, ascii_uppercase
for k in range(1, 100):
    for l in ascii_lowercase:
        d.__setattr__(l*k, l*k)
sys.getsizeof(d)
```

```{python}
from fastai.vision.models import DynamicUnet
from fastai.vision.learner import unet_learner
from torchvision.models import resnet34
import collections
from typing import Callable
import functools

def _default_split(m:nn.Module): return (m[1],)
# Split a resnet style model
def _resnet_split(m:nn.Module): return (m[0][6],m[1])
# Split squeezenet model on maxpool layers
def _squeezenet_split(m:nn.Module): return (m[0][0][5], m[0][0][8], m[1])
def _densenet_split(m:nn.Module): return (m[0][0][7],m[1])
def _vgg_split(m:nn.Module): return (m[0][0][22],m[1])
def _alexnet_split(m:nn.Module): return (m[0][0][6],m[1])

_default_meta    = {'cut':None, 'split':_default_split}
_resnet_meta     = {'cut':-2, 'split':_resnet_split }
_squeezenet_meta = {'cut':-1, 'split': _squeezenet_split}
_densenet_meta   = {'cut':-1, 'split':_densenet_split}
_vgg_meta        = {'cut':-1, 'split':_vgg_split}
_alexnet_meta = {'cut':-1, 'split':_alexnet_split}

model_meta = {
    resnet34: {**_resnet_meta}}

def children(m):
    "Get children of `m`."
    return list(m.children())

def num_children(m):
    "Get number of children modules in `m`."
    return len(children(m))

class PrePostInitMeta(type):
    "A metaclass that calls optional `__pre_init__` and `__post_init__` methods"
    def __new__(cls, name, bases, dct):
        x = super().__new__(cls, name, bases, dct)
        old_init = x.__init__
        def _pass(self): pass
        @functools.wraps(old_init)
        def _init(self,*args,**kwargs):
            self.__pre_init__()
            old_init(self, *args,**kwargs)
            self.__post_init__()
        x.__init__ = _init
        if not hasattr(x,'__pre_init__'):  x.__pre_init__  = _pass
        if not hasattr(x,'__post_init__'): x.__post_init__ = _pass
        return x

class Module(nn.Module, metaclass=PrePostInitMeta):
    "Same as `nn.Module`, but no need for subclasses to call `super().__init__`"
    def __pre_init__(self): super().__init__()
    def __init__(self): pass

class ParameterModule(Module):
    "Register a lone parameter `p` in a module."
    def __init__(self, p): self.val = p
    def forward(self, x): return x

def children_and_parameters(m):
    "Return the children of `m` and its direct parameters not registered in modules."
    children = list(m.children())
    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()],[])
    for p in m.parameters():
        if id(p) not in children_p: children.append(ParameterModule(p))
    return children

flatten_model = lambda m: sum(map(flatten_model,children_and_parameters(m)),[]) if num_children(m) else [m]


def listify(p=None, q=None):
    "Make `p` listy and the same length as `q`."
    if p is None: p=[]
    elif isinstance(p, str):          p = [p]
    elif not isinstance(p, collections.Iterable): p = [p]
    #Rank 0 tensors in PyTorch are Iterable but don't have a length.
    else:
        try: a = len(p)
        except: p = [p]
    n = q if type(q)==int else len(p) if q is None else len(q)
    if len(p)==1: p = p * n
    assert len(p)==n, f'List len mismatch ({len(p)} vs {n})'
    return list(p)

def cnn_config(arch):
    "Get the metadata associated with `arch`."
    torch.backends.cudnn.benchmark = True
    return model_meta.get(arch, _default_meta)

def cond_init(m, init_fn):
    if not isinstance(m, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d)):
        if hasattr(m, 'weight'): init_fn(m.weight)            
        if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)
            
def apply_init(m, init_fn):
    m.apply(lambda x: cond_init(x, init_fn))

def split_model_idx(model, idxs):
    "Split `model` according to the indexes in `idxs`."
    layers = flatten_model(model)
    if idxs[0] != 0: idxs = [0] + idxs
    if idxs[-1] != len(layers): idxs.append(len(layers))
    return [nn.Sequential(*layers[i:j]) for i,j in zip(idxs[:-1],idxs[1:])]

def first_layer(m):
    "Retrieve first layer in a module `m`."
    return flatten_model(m)[0]

def split_model(model=None, splits=None):
    "Split `model` according to the layers in `splits`."
    splits = listify(splits)
    if isinstance(splits[0], nn.Module):
        layers = flatten_model(model)
        idxs = [layers.index(first_layer(s)) for s in splits]
        return split_model_idx(model, idxs)
    return [nn.Sequential(*s) for s in splits]

meta = cnn_config(resnet34)
split_on = meta['split']
unet = DynamicUnet(create_body(resnet34, pretrained=False), 1)
if isinstance(split_on,Callable): split_on = split_on(unet)
split_model(unet, split_on)
apply_init(unet[2], nn.init.kaiming_normal_)
```

```{python}
unet
```

```{python}
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

opt = optim.Adam(unet.parameters(), lr=cfg.LRS[0], weight_decay=cfg.WD, betas=(0.9, 0.99))
net = Net(unet, opt, nn.BCEWithLogitsLoss(), [mean_iou], cfg.MODELS_PATH)
```

```{python}
scheduler = OneCycleScheduler(cfg.LRS, len(tl))
save_name = f'{cfg.MODEL}_fastai_{cfg.EPOCHS}_{cfg.LRS[0]}_{cfg.WD}'
save_name += f'_{getNextFilePath(cfg.MODELS_PATH, save_name)}'
print(save_name)
```

```{python}
writer = SummaryWriter(log_dir='/work/stages/schwob/runs/'+save_name)
```

```{python}
writer.add_graph(unet, input_to_model=next(iter(tl))[0], operator_export_type='RAW')
```

```{python}
net.fit(dls, cfg.EPOCHS, save_name, device, writer=writer, scheduler=scheduler)
```

```{python}
import collections
def is_iter(x): return isinstance(x, collections.Iterable)
def listify(x, y):
    if not is_iter(x): x=[x]
    n = y if type(y)==int else len(y)
    if len(x)==1: x = x * n
    return x
```

```{python}
listify(0, [0.1, 0.1])
```

```{python}

```
